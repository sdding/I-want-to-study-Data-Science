{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "원티드 프리온보딩 코스.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPx1N2T5VR0btqx4f/h3XsG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdding/I-want-to-study-Data-Science/blob/master/%EC%9B%90%ED%8B%B0%EB%93%9C_%ED%94%84%EB%A6%AC%EC%98%A8%EB%B3%B4%EB%94%A9_%EC%BD%94%EC%8A%A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 원티드 프리온보딩 AL/ML 코스 사전과제"
      ],
      "metadata": {
        "id": "ZdmZmjuDQpET"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8e3TzVu8dkL7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "\n",
        "  def preprocessing(self, sequences):\n",
        "    result= []\n",
        "    # 문제 1-1\n",
        "    for text in sequences:  \n",
        "      token = re.sub(r'[^0-9a-zA-Z]', ' ', str(text)).lower().strip()   # 소문자 변환, 특수문자 제거, 토큰화\n",
        "      result.append(token.split(' '))\n",
        "          \n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    # 문제 1-2\n",
        "    tokens = self.preprocessing(sequences) # 문장에 대해 토큰화\n",
        "    \n",
        "    word_list = []\n",
        "    for token in tokens:\n",
        "      word_list.append(token)\n",
        "\n",
        "    i = 1                           # 1번부터 정수 인덱싱\n",
        "    for token in tokens:\n",
        "      for t in sorted(token):\n",
        "        if t not in self.word_dict:  # 토큰이 사전에 없으면 사전에 추가(중복 방지)\n",
        "          self.word_dict[t] = i\n",
        "          i += 1\n",
        "\n",
        "    self.fit_checker = True\n",
        "    \n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences)    \n",
        "    if self.fit_checker:\n",
        "      # 문제 1-3\n",
        "      for token in tokens:\n",
        "        token_list = []\n",
        "        for t in token:\n",
        "          if t not in self.word_dict:  # 어휘 사전에 없는 단어는 'oov'의 index 0 으로 변환\n",
        "            token_list.append(self.word_dict['oov'])\n",
        "          else:\n",
        "            token_list.append(self.word_dict[t])  # 사전에 있다면 해당 인덱스로 변환\n",
        "        result.append(token_list)\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "o_peuUwghuqs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tokenizer().fit_transform(['I go to school.', 'I LIKE pizza!'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWVKhK1ViNPa",
        "outputId": "470bb3f3-67d0-4b30-f857-3b3c1a838c64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1, 4, 3], [2, 5, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer: \n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    # 문제 2-1\n",
        "    M = max(map(max, tokenized))  # 토큰 숫자중 최대값\n",
        "    global IDF\n",
        "    IDF = []\n",
        "    \n",
        "    for i in range(1, M+1):\n",
        "      count = 0\n",
        "      for tokens in tokenized:\n",
        "        if i in tokens:\n",
        "          count += 1\n",
        "      IDF.append(np.log(len(tokenized) / (1+count)))  # IDF 값\n",
        "    \n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      # 문제 2-2\n",
        "      M = max(map(max, tokenized))  # 토큰 숫자중 최대값\n",
        "      TF = []\n",
        "      for tokens in tokenized:\n",
        "        count = []\n",
        "        for i in range(1, M+1):\n",
        "          count.append(tokens.count(i))\n",
        "        TF.append(count)\n",
        "        \n",
        "      self.tfidf_matrix = []\n",
        "      for i in TF:\n",
        "        self.tfidf_matrix.append(list(np.multiply(i, IDF))) # tf-idf 계산\n",
        "\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "EGND3k9yo9a2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TfidfVectorizer(Tokenizer()).fit_transform(['I go to school.', 'I LIKE pizza!'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQl1XuYI53ms",
        "outputId": "f4f4f1ed-1dd2-451b-8738-4da61f4c11cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0, -0.40546510810816444, 0.0, 0.0, 0.0, 0.0],\n",
              " [0.0, -0.40546510810816444, 0.0, 0.0, 0.0, 0.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}